<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
    <dict>
        <key>name</key>
        <string>Generic Profile</string>
        <key>identifier</key>
        <string>com.hugomelder.software</string>
        <key>version</key>
        <string>0.1.0</string>
        <key>description</key>
        <string>Generic pipeline profile for all platforms.</string>
        <key>supportedPlatforms</key>
        <array>
            <string>all</string>
        </array>
        
        <!--
        Mountpoints and the underlying GStreamer pipelines are managed
        by the GStreamer RTSP server.
        
        You can choose between various types of mountpoints in the config.plist
        (currently single, and combined), but every mountpoint needs a GStreamer
        pipeline description.
        
        Pipeline configurations for mountpoints must contain rtp payloaders as
        sink elements in the format pay%d, where %d is a non-negative integer
        (Further information can be found in the GStreamer RTSP Server
        documentation).
        
        We use variables enclosed in '{}' for values that are populated during pipelines
        construction.
        
        Currently, the following variables are available:
        - {VIDEOCHANNEL.%u}: The video channel name. Enumerated using unsigned
        integers, starting at 0 (e.g. {VIDEOCHANNEL.0})
        - {PULSEDEV}: The pulse audio device name
        (e.g. alsa_input.pci-0000_00_03.0.analog-stereo)
        
        'pactl list sources short' lists all availables sources. The second column
        in the table is the pulseaudio device name.
        Replace 'sources' with 'sinks' to get a list of all available sinks.
        -->
        <key>mountpoints</key>
        <dict>
            <key>single</key>
            <!--
                intervideo{src,sink} is a mechanism to share buffers and events across independent
                pipelines. We connect to the channel {VIDEOCHANNEL.0} and do the postprocessing
                and h264 encoding on CPU (using libx264). The resulting h264 stream is then fed
                into the rtp payloader.
            -->
            <string>intervideosrc channel={VIDEOCHANNEL.0} ! queue ! video/x-raw,width=1920,height=1080 !
 videoconvert ! x264enc bitrate=2500 ! rtph264pay name=pay0 pt=96</string>
            <key>combined</key>
            <string>compositor name=comp background=1
 sink_0::xpos=0 sink_0::ypos=0 sink_0::width=1440 sink_0::height=810 sink_0::sizing-policy=1
 sink_1::xpos=1440 sink_1::ypos=0 sink_1::width=480 sink_1::height=270 sink_1::sizing-policy=1 !
 video/x-raw,width=1920,height=1080 ! x264enc bitrate=2500 ! rtph264pay name=pay0 pt=96
 intervideosrc channel={VIDEOCHANNEL.0} ! queue ! comp.sink_0
 intervideosrc channel={VIDEOCHANNEL.1} ! queue ! comp.sink_1</string>
        </dict>
        
        <key>channels</key>
        <dict>
            <key>v4l2</key>
            <string>v4l2src device={V4L2DEV} ! videoconvertscale add-borders=1 ! video/x-raw, width=1920, height=1080 ! queue ! intervideosink channel={VIDEOCHANNEL.0}</string>
            <key>decklink</key>
            <string>decklinksrc device-number={DEV} ! videoconvert ! videoscale ! videorate ! video/x-raw, width=1920, height=1080 ! intervideosink channel={VIDEOCHANNEL.0}</string>
            <key>videoTest</key>
            <string>videotestsrc is-live=1 ! video/x-raw,width={WIDTH},height={HEIGHT},format=NV12 !
 intervideosink channel={VIDEOCHANNEL.0}</string>
        </dict>
        <key>audioProviders</key>
        <dict>
            <key>pulse</key>
            <string>pulsesrc device={PULSEDEV} ! voaacenc bitrate=96000 ! queue ! rtpmp4apay name=pay1 pt=97</string>
            <key>audioTest</key>
            <string>audiotestsrc ! voaacenc bitrate=96000 ! queue ! rtpmp4apay name=pay1 pt=97</string>
        </dict>

        <!--
            Configuration for video and audio recording.  Partial (pipelines)
            are used to describe device-dependent encoding and are pieced
            together in a VMPRecordingManager.
        -->
        <key>recordings</key>
        <dict>
            <!--
                Convert, Scale, and Encode a video channel feed into h264.

                Variables:
                - {VIDEOCHANNEL}: The video channel
                - {WIDTH}: Width after scaling
                - {HEIGHT}: Height after scaling
                - {BITRATE}: h264 encoding bitrate in kbps
            -->
            <key>video</key>
            <string>intervideosrc channel={VIDEOCHANNEL} ! queue !
 videoconvertscale add-borders=1 ! video/x-raw, width={WIDTH}, height={HEIGHT} ! x264enc bitrate={BITRATE}</string>
            <!--
                Open a pulseaudio source and encode it as AAC LC.

                Variables:
                - {PULSEDEV}: PulseAudio Device (See `audioProviders` for detailed information)
                - {BITRATE}: AAC Audio Bitrate (bits per second)
            -->
            <key>pulse</key>
            <string>pulsesrc device={PULSEDEV} ! voaacenc bitrate={BITRATE}</string>
        </dict>
    </dict>
</plist>
